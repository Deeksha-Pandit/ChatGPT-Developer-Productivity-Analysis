{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 2"
      ],
      "metadata": {
        "id": "aWr1073Lvg-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part A"
      ],
      "metadata": {
        "id": "EWd42mYc2Wf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt: Generate code for sentiment analysis"
      ],
      "metadata": {
        "id": "rvdp_Fjo2slO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# initialize the sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# example text to analyze\n",
        "text = \"I love this product! It's amazing.\"\n",
        "\n",
        "# analyze the sentiment of the text\n",
        "scores = sia.polarity_scores(text)\n",
        "\n",
        "# print the sentiment scores\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "YlFziy4u2Ysk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt: add topic modeling into the function above"
      ],
      "metadata": {
        "id": "_XVFualh2ybB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# initialize the sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def analyze_sentiment_and_topics(text):\n",
        "    # analyze the sentiment of the text\n",
        "    sentiment_scores = sia.polarity_scores(text)\n",
        "    \n",
        "    # tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    \n",
        "    # create a bag-of-words representation of the tokens\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(tokens)\n",
        "    \n",
        "    # perform topic modeling using Latent Dirichlet Allocation (LDA)\n",
        "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "    lda.fit(X)\n",
        "    topics = lda.components_\n",
        "    \n",
        "    # return the sentiment scores and topics\n",
        "    return sentiment_scores, topics\n"
      ],
      "metadata": {
        "id": "1xUHXh-t20cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tp = analyze_sentiment_and_topics(text)\n",
        "tp"
      ],
      "metadata": {
        "id": "depxbCWJ220Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt: add tokenize feature into the code above and increase the complexity of the code"
      ],
      "metadata": {
        "id": "GPabVd8N24zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# initialize the sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# initialize the lemmatizer and stopwords list\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "def analyze_sentiment_and_topics(text):\n",
        "    # tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    \n",
        "    # analyze the sentiment of each sentence\n",
        "    sentiment_scores = []\n",
        "    for sentence in sentences:\n",
        "        # tokenize the sentence and remove stopwords and punctuation\n",
        "        tokens = [lemmatizer.lemmatize(token.lower()) for token in word_tokenize(sentence) \n",
        "                  if token.lower() not in stopwords and token.isalnum()]\n",
        "        \n",
        "        # analyze the sentiment of the sentence\n",
        "        sentence_sentiment = sia.polarity_scores(' '.join(tokens))\n",
        "        sentiment_scores.append(sentence_sentiment)\n",
        "    \n",
        "    # create a bag-of-words representation of the tokens\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform([' '.join(tokens) for tokens in sentences])\n",
        "    \n",
        "    # perform topic modeling using Latent Dirichlet Allocation (LDA)\n",
        "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "    lda.fit(X)\n",
        "    topics = lda.components_\n",
        "    \n",
        "    # return the sentiment scores and topics\n",
        "    return sentiment_scores, topics\n"
      ],
      "metadata": {
        "id": "sJg7OtYQ264e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X6c7758fvf8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt: transform the code above to do function that only do sentiment analysis on sentences contains snacks  brands:"
      ],
      "metadata": {
        "id": "CmIa9LjIiVow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# initialize the sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# initialize the lemmatizer and stopwords list\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "# define a list of snack brands\n",
        "snack_brands = ['lays', 'doritos', 'cheetos', 'pringles', 'fritos']\n",
        "\n",
        "def analyze_sentiment_snack_sentences(text):\n",
        "    # tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    \n",
        "    # analyze the sentiment of sentences that mention snack brands\n",
        "    sentiment_scores = []\n",
        "    for sentence in sentences:\n",
        "        # check if the sentence mentions any snack brands\n",
        "        snack_mentioned = any([brand in sentence.lower() for brand in snack_brands])\n",
        "        \n",
        "        if snack_mentioned:\n",
        "            # tokenize the sentence and remove stopwords and punctuation\n",
        "            tokens = [lemmatizer.lemmatize(token.lower()) for token in word_tokenize(sentence) \n",
        "                      if token.lower() not in stopwords and token.isalnum()]\n",
        "\n",
        "            # analyze the sentiment of the sentence\n",
        "            sentence_sentiment = sia.polarity_scores(' '.join(tokens))\n",
        "            sentence_sentiment['sentence'] = sentence\n",
        "            sentiment_scores.append(sentence_sentiment)\n",
        "    \n",
        "    # return the sentiment scores\n",
        "    return sentiment_scores\n"
      ],
      "metadata": {
        "id": "IyrAnnuJiiyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt: transform the code above that find the sentiment score of each snacks in a text file and calcute the lift value for each snack:"
      ],
      "metadata": {
        "id": "2d7MgaDaiuCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# initialize the sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# initialize the lemmatizer and stopwords list\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "# define a list of snack brands\n",
        "snack_brands = ['lays', 'doritos', 'cheetos', 'pringles', 'fritos']\n",
        "\n",
        "def calculate_lift_value(file_path):\n",
        "    # read the contents of the file\n",
        "    with open(file_path, 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # create a dictionary to store the frequency of each snack brand in the text\n",
        "    snack_frequencies = {brand: 0 for brand in snack_brands}\n",
        "\n",
        "    # loop through each sentence in the text\n",
        "    for sentence in sentences:\n",
        "        # loop through each snack brand\n",
        "        for brand in snack_brands:\n",
        "            # check if the sentence mentions the snack brand\n",
        "            if brand in sentence.lower():\n",
        "                snack_frequencies[brand] += 1\n",
        "\n",
        "    # calculate the total number of sentences in the text\n",
        "    total_sentences = len(sentences)\n",
        "\n",
        "    # calculate the lift value of each snack brand\n",
        "    snack_lift_values = {}\n",
        "    for brand in snack_brands:\n",
        "        # calculate the frequency of the snack brand in the text\n",
        "        snack_frequency = snack_frequencies[brand]\n",
        "\n",
        "        # calculate the frequency of the snack brand in the absence of the text\n",
        "        non_snack_frequency = total_sentences - snack_frequency\n",
        "\n",
        "        # calculate the frequency of all other snack brands in the text\n",
        "        other_snack_frequencies = sum([snack_frequencies[b] for b in snack_brands if b != brand])\n",
        "\n",
        "        # calculate the frequency of all other words in the text\n",
        "        all_words = word_tokenize(text.lower())\n",
        "        all_word_frequencies = Counter(all_words)\n",
        "        other_word_frequencies = sum([all_word_frequencies[w] for w in all_word_frequencies.keys() \n",
        "                                      if w not in snack_brands and w not in stopwords])\n",
        "\n",
        "        # calculate the lift value of the snack brand\n",
        "        lift_value = (total_sentences * snack_frequency) / (snack_frequency + other_snack_frequencies) \\\n",
        "                     * (snack_frequency + non_snack_frequency) / (non_snack_frequency + other_snack_frequencies) \\\n",
        "                     * (snack_frequency + other_snack_frequencies) / (snack_frequency + other_snack_frequencies + other_word_frequencies)\n",
        "\n",
        "        # round the lift value to 2 decimal places\n",
        "        lift_value = round(lift_value, 2)\n",
        "\n",
        "        # add the lift value to the dictionary\n",
        "        snack_lift_values[brand] = lift_value\n",
        "\n",
        "    # return the lift values of each snack brand\n",
        "    return snack_lift_values\n"
      ],
      "metadata": {
        "id": "Zrn0hb_jitO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The code above has an error where it misses the case that the same brand of the snack got mention multiple times in a sentence. So if a review sentence start with \"Lays, Lays, Lays\", it will count Lays three times instead of 1 which will make the result inaccurate."
      ],
      "metadata": {
        "id": "wlvtYl85jEd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B. Code Improvements with ChatGPT\n",
        "\n",
        "#### To begin with we picked up some sample code from nltk on VADER sentiment \n",
        "#### analysis. We made the code very choppy by adding unecessary import \n",
        "#### statements all over the place. We also changed variable names from \n",
        "#### decriptive to very vague names like var1, var2. The code was very\n",
        "#### slopy to begin with and we were using depracated ways of doing things\n",
        "#### which was in the sample code to begin with. The initial code is in the \n",
        "#### cell below."
      ],
      "metadata": {
        "id": "xeo8UPOuNcQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "nltk.download('subjectivity')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "n_instances = 100\n",
        "var1 = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
        "var2 = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
        "len(var1), len(var2)\n",
        "var1[0]\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import subjectivity\n",
        "\n",
        "\n",
        "var3 = var1[:80]\n",
        "var4 = var1[80:100]\n",
        "var5 = var2[:80]\n",
        "var6 = var2[80:100]\n",
        "var7 = var3 + var5\n",
        "var8 = var4 + var6\n",
        "sentim_analyzer = SentimentAnalyzer()\n",
        "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in var7])\n",
        "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
        "len(unigram_feats)\n",
        "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
        "training_set = sentim_analyzer.apply_features(var7)\n",
        "test_set = sentim_analyzer.apply_features(var8)\n",
        "trainer = NaiveBayesClassifier.train\n",
        "classifier = sentim_analyzer.train(trainer, training_set)\n",
        "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
        "     print('{0}: {1}'.format(key, value))\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "\n",
        "\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sentences = [\"VADER is smart, handsome, and funny.\", # positive sentence example\n",
        "                  \"VADER is smart, handsome, and funny!\", # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
        "                 \"VADER is very smart, handsome, and funny.\"\n",
        "                  ]\n",
        "paragraph = \"It was one of the worst movies I've seen, despite good reviews. \\\n",
        " Unbelievably bad acting!! Poor direction. VERY poor production. \\\n",
        " The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!\"\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "\n",
        "\n",
        "\n",
        "lines_list = tokenize.sent_tokenize(paragraph)\n",
        "sentences.extend(lines_list)\n",
        "\n",
        "\n",
        "tricky_sentences = [\n",
        "         \"Most automated sentiment analysis tools are shit.\",\n",
        "         \"VADER sentiment analysis is the shit.\",\n",
        "     \"\",\n",
        "         \"Sentiment analysis has never been good.\"\n",
        "\n",
        "      ]\n",
        "sentences.extend(tricky_sentences)\n",
        "for sentence in sentences:\n",
        "     sid = SentimentIntensityAnalyzer()\n",
        "     print(sentence)\n",
        "     ss = sid.polarity_scores(sentence)\n",
        "     for k in sorted(ss):\n",
        "              print('{0}: {1}, '.format(k, ss[k]), end='')\n",
        "     print()"
      ],
      "metadata": {
        "id": "_AfjyPMsNXu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ChatGPT improved this code a lot and removed all the unnecessary import\n",
        "#### statements. It also removed some instances of using old depracted \n",
        "#### string format methods with newer modern ones. It also used a more \n",
        "#### efficient method to tokenize strings. It also did some cleanup and added\n",
        "#### indentation where needed. it did not however, change the variable names"
      ],
      "metadata": {
        "id": "XV8Z3zl6bRUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import mark_negation\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('subjectivity')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "n_instances = 100\n",
        "\n",
        "var1 = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
        "var2 = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
        "\n",
        "var3 = var1[:80]\n",
        "var4 = var1[80:100]\n",
        "var5 = var2[:80]\n",
        "var6 = var2[80:100]\n",
        "var7 = var3 + var5\n",
        "var8 = var4 + var6\n",
        "\n",
        "sentim_analyzer = SentimentAnalyzer()\n",
        "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in var7])\n",
        "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
        "\n",
        "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
        "\n",
        "training_set = sentim_analyzer.apply_features(var7)\n",
        "test_set = sentim_analyzer.apply_features(var8)\n",
        "\n",
        "trainer = NaiveBayesClassifier.train\n",
        "classifier = sentim_analyzer.train(trainer, training_set)\n",
        "\n",
        "for key, value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
        "     print('{0}: {1}'.format(key, value))\n",
        "\n",
        "sentences = [\"VADER is smart, handsome, and funny.\", # positive sentence example\n",
        "                  \"VADER is smart, handsome, and funny!\", # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
        "                 \"VADER is very smart, handsome, and funny.\"\n",
        "                  ]\n",
        "paragraph = \"It was one of the worst movies I've seen, despite good reviews. \\\n",
        "Unbelievably bad acting!! Poor direction. VERY poor production. \\\n",
        "The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!\"\n",
        "\n",
        "lines_list = sent_tokenize(paragraph)\n",
        "sentences.extend(lines_list)\n",
        "\n",
        "tricky_sentences = [\n",
        "         \"Most automated sentiment analysis tools are shit.\",\n",
        "         \"VADER sentiment analysis is the shit.\",\n",
        "         \"\",\n",
        "         \"Sentiment analysis has never been good.\"\n",
        "]\n",
        "\n",
        "sentences.extend(tricky_sentences)\n",
        "\n",
        "for sentence in sentences:\n",
        "     sid = SentimentIntensityAnalyzer()\n",
        "     print(sentence)\n",
        "     ss = sid.polarity_scores(sentence)\n",
        "     for k, v in sorted(ss.items()):\n",
        "         print(f\"{k}: {v}, \", end='')\n",
        "     print()\n"
      ],
      "metadata": {
        "id": "jg9lgae-bfqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Upon asking it to improve further, ChatGPT did change the ambiguous \n",
        "#### variable names to more descriptive ones. It also changed the string\n",
        "#### formatting method again to a supposedly more easier to read one. We think \n",
        "#### this was not necessary and the .format() it changed previously was more\n",
        "#### than sufficient. It again did some cleanup and moved some imports into \n",
        "#### one line and added some new lines for readability. There was overall no \n",
        "#### change in logic."
      ],
      "metadata": {
        "id": "IaXnBy6ab-zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import mark_negation\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('subjectivity')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "n_instances = 100\n",
        "\n",
        "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
        "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
        "\n",
        "train_docs = subj_docs[:80] + obj_docs[:80]\n",
        "test_docs = subj_docs[80:100] + obj_docs[80:100]\n",
        "\n",
        "sentim_analyzer = SentimentAnalyzer()\n",
        "\n",
        "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in train_docs])\n",
        "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
        "\n",
        "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
        "\n",
        "training_set = sentim_analyzer.apply_features(train_docs)\n",
        "test_set = sentim_analyzer.apply_features(test_docs)\n",
        "\n",
        "trainer = NaiveBayesClassifier.train\n",
        "classifier = sentim_analyzer.train(trainer, training_set)\n",
        "\n",
        "for key, value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
        "     print(f'{key}: {value}')\n",
        "\n",
        "sentences = [\"VADER is smart, handsome, and funny.\", # positive sentence example\n",
        "             \"VADER is smart, handsome, and funny!\", # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
        "             \"VADER is very smart, handsome, and funny.\"\n",
        "            ]\n",
        "\n",
        "paragraph = \"It was one of the worst movies I've seen, despite good reviews. \\\n",
        "Unbelievably bad acting!! Poor direction. VERY poor production. \\\n",
        "The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!\"\n",
        "\n",
        "lines_list = sent_tokenize(paragraph)\n",
        "sentences.extend(lines_list)\n",
        "\n",
        "tricky_sentences = [\n",
        "         \"Most automated sentiment analysis tools are shit.\",\n",
        "         \"VADER sentiment analysis is the shit.\",\n",
        "         \"\",\n",
        "         \"Sentiment analysis has never been good.\"\n",
        "]\n",
        "\n",
        "sentences.extend(tricky_sentences)\n",
        "\n",
        "for sentence in sentences:\n",
        "     sid = SentimentIntensityAnalyzer()\n",
        "     ss = sid.polarity_scores(sentence)\n",
        "     for k, v in sorted(ss.items()):\n",
        "         print(f\"{k}: {v}, \", end='')\n",
        "     print()\n"
      ],
      "metadata": {
        "id": "zF0Vpxs7dWSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We asked it again to improve the code and this time it yet again said it \n",
        "#### changed the string formatting method from using the depracated % to \n",
        "#### .format() but the code we gave it to improve was the same code we got from \n",
        "#### the previous step which didnt have the % string formatter. It also \n",
        "#### expanded the previously written variable names to more descriptive ones.\n",
        "#### Which was not needed but still does improve readability. Finally, it \n",
        "#### did add comments for each relevant line of code. There was no logic change"
      ],
      "metadata": {
        "id": "1zkM9eLwddNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import mark_negation\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('subjectivity')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Set number of instances\n",
        "n_instances = 100\n",
        "\n",
        "# Load subjectivity and objectivity sentences\n",
        "subjectivity_sentences = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
        "objectivity_sentences = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_docs = subjectivity_sentences[:80] + objectivity_sentences[:80]\n",
        "test_docs = subjectivity_sentences[80:100] + objectivity_sentences[80:100]\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "sentim_analyzer = SentimentAnalyzer()\n",
        "\n",
        "# Extract unigram features\n",
        "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in train_docs])\n",
        "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
        "\n",
        "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
        "\n",
        "# Apply feature extractor to training and testing sets\n",
        "training_set = sentim_analyzer.apply_features(train_docs)\n",
        "test_set = sentim_analyzer.apply_features(test_docs)\n",
        "\n",
        "# Train Naive Bayes classifier on training set\n",
        "trainer = NaiveBayesClassifier.train\n",
        "classifier = sentim_analyzer.train(trainer, training_set)\n",
        "\n",
        "# Evaluate classifier on testing set\n",
        "for key, value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
        "     print(f'{key}: {value}')\n",
        "\n",
        "# Analyze sentiment of sample sentences\n",
        "sentences = [\n",
        "    \"VADER is smart, handsome, and funny.\", # positive sentence example\n",
        "    \"VADER is smart, handsome, and funny!\", # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
        "    \"VADER is very smart, handsome, and funny.\"\n",
        "]\n",
        "\n",
        "paragraph = \"It was one of the worst movies I've seen, despite good reviews. \\\n",
        "Unbelievably bad acting!! Poor direction. VERY poor production. \\\n",
        "The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!\"\n",
        "\n",
        "lines_list = sent_tokenize(paragraph)\n",
        "sentences.extend(lines_list)\n",
        "\n",
        "tricky_sentences = [\n",
        "    \"Most automated sentiment analysis tools are shit.\",\n",
        "    \"VADER sentiment analysis is the shit.\",\n",
        "    \"\",\n",
        "    \"Sentiment analysis has never been good.\"\n",
        "]\n",
        "\n",
        "sentences.extend(tricky_sentences)\n",
        "\n",
        "for sentence in sentences:\n",
        "     sid = SentimentIntensityAnalyzer()\n",
        "     ss = sid.polarity_scores(sentence)\n",
        "     for k, v in sorted(ss.items()):\n",
        "         print(f\"{k}: {v}, \", end='')\n",
        "     print()\n"
      ],
      "metadata": {
        "id": "01_4NH1mdiYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bCiSKD5uhoim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bold text**## Part B. Adding complexity "
      ],
      "metadata": {
        "id": "smaDxEX_NRPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Then we added more conplexity to our code. Following the following procedure\n",
        "\n",
        "\n",
        "1. Added more data: Changed the amount of data used from 100 instances to 5000 instances.\n",
        "\n",
        "2. The pre-processing steps used: the nltk library is used to perform tokenization and lemmatization on the text data, which can help improve the quality of the features used in the model. \n",
        "\n",
        "3. Added a new classifier: In addition to the Naive Bayes classifier, we added a Support Vector Machine (SVM) classifier to the pipeline. The SVM classifier is known to work well on text classification tasks and can handle high-dimensional feature spaces. Also, We added TF-IDF to assign weights to each word in the text and capture the importance of rare words. Finally, *mark_negation* is applied to the preprocessed text data before the TF-IDF feature extraction, which an improvement compared to the previous version, as it can help the model better capture the nuances of negated sentiments.\n",
        "\n",
        "\n",
        "###After adding the complexity, the code looks as follows:\n"
      ],
      "metadata": {
        "id": "sBBDjLxZUzm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.classify import SklearnClassifier\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.util import mark_negation\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('subjectivity')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Set number of instances\n",
        "n_instances = 5000\n",
        "\n",
        "# Load subjectivity and objectivity sentences\n",
        "subjectivity_sentences = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
        "objectivity_sentences = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_docs = subjectivity_sentences[:4000] + objectivity_sentences[:4000]\n",
        "test_docs = subjectivity_sentences[4000:] + objectivity_sentences[4000:]\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "vectorizer = TfidfVectorizer(tokenizer=word_tokenize, preprocessor=lemmatizer.lemmatize)\n",
        "classifier = SklearnClassifier(Pipeline([('tfidf', vectorizer), ('svm', SVC(kernel='linear'))]))\n",
        "\n",
        "# Apply feature extractor to training and testing sets\n",
        "train_data = [mark_negation(doc) for doc in train_docs]\n",
        "train_X = vectorizer.fit_transform([doc for doc, _ in train_data])\n",
        "train_y = [label for _, label in train_data]\n",
        "\n",
        "test_data = [mark_negation(doc) for doc in test_docs]\n",
        "test_X = vectorizer.transform([doc for doc, _ in test_data])\n",
        "test_y = [label for _, label in test_data]\n",
        "\n",
        "# Train classifier on training set\n",
        "classifier.train(zip(train_X, train_y))\n",
        "\n",
        "# Evaluate classifier on testing set\n",
        "accuracy = nltk.classify.accuracy(classifier, zip(test_X, test_y))\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Analyze sentiment of sample sentences\n",
        "sentences = [\n",
        "    \"VADER is smart, handsome, and funny.\", # positive sentence example\n",
        "    \"VADER is smart, handsome, and funny!\", # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
        "    \"VADER is very smart, handsome, and funny.\"\n",
        "]\n",
        "\n",
        "paragraph = \"It was one of the worst movies I've seen, despite good reviews. \\\n",
        "Unbelievably bad acting!! Poor direction. VERY poor production. \\\n",
        "The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!\"\n",
        "\n",
        "lines_list = nltk.sent_tokenize(paragraph)\n",
        "sentences.extend(lines_list)\n",
        "\n",
        "tricky_sentences = [\n",
        "    \"Most automated sentiment analysis tools are shit.\",\n",
        "    \"VADER sentiment analysis is the shit.\",\n",
        "    \"\",\n",
        "    \"Sentiment analysis has never been good.\"\n",
        "]\n",
        "\n",
        "sentences.extend(tricky_sentences)\n",
        "\n",
        "for sentence in sentences:\n",
        "     sid = SentimentIntensityAnalyzer()\n",
        "     ss = sid.polarity_scores(sentence)\n",
        "     print(f'{sentence}\\nPositive: {ss[\"pos\"]:.4f}\\nNegative: {ss[\"neg\"]:.4f}\\nNeutral: {ss[\"neu\"]:.4f}\\n')\n"
      ],
      "metadata": {
        "id": "uCLiHzMHaRhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Does ChatGPT handle complexity\n",
        "\n",
        "#### After asking ChatGPT to improve this code, ChatGPT did not give us a code\n",
        "#### block. It simply gave us tips on how to improve it further. For example it\n",
        "#### asked us to use more data which is not exactly improving the code. We then \n",
        "#### specifically asked ChatGPT to improve the code and not explain what it did.\n",
        "#### This was an interesting occurance because it gave us the same code back \n",
        "#### unchanged. When we asked it to explain what it did, it said that it did \n",
        "#### not change anything because we asked it not explain anything. It was \n",
        "#### almost as if it was being sassy with us. After that we changed what we \n",
        "#### asked in the prompt to \"improve this piece of code and keep explanations \n",
        "#### of improvements concise\" it gave us some general improvements again like\n",
        "#### removing unused imports. But it also asked us to use Grid Search to \n",
        "#### improve our svc classifier. Which is a step in the right direction but\n",
        "#### it is a very general advice which programmers/data scientists would \n",
        "#### do in their field to explore the best paramaters (Hyper parameter tuning).\n",
        "#### In conclusion, we can say that for general low level improvements like \n",
        "#### removing unused imports and changing variable names ChatGPT can be helpful.\n",
        "#### However, these improvements are better suited for IDE suggestions and code\n",
        "#### scan tools like SonarQube. For more complex code ChatGPT cannot add much \n",
        "#### besides suggesting general best practices.\n"
      ],
      "metadata": {
        "id": "be5QIMQ03C22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**PART C**\n",
        "\n",
        "#### For this part of the assignment, we gave ChatGPT a NLP code snippets and checked if it was able to explain what the code is about."
      ],
      "metadata": {
        "id": "TncZQA1BkjwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using ChatGpt to understand Text Classification with BERT in PyTorch\n",
        "\n",
        "Referred the following github repo - https://github.com/nlptown/nlp-notebooks/blob/master/Text%20classification%20with%20BERT%20in%20PyTorch.ipynb "
      ],
      "metadata": {
        "id": "Y9NwKuAMl6tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt1\n",
        "import ndjson\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open(CORPUS_PATH) as i:\n",
        "    data = ndjson.load(i)\n",
        "    \n",
        "texts = [\" \".join([doc[\"title\"], doc[\"body\"]]) for doc in data]\n",
        "labels = [doc[\"rating\"] for doc in data]\n",
        "    \n",
        "rest_texts, test_texts, rest_labels, test_labels = train_test_split(texts, labels, test_size=0.1, random_state=1)\n",
        "train_texts, dev_texts, train_labels, dev_labels = train_test_split(rest_texts, rest_labels, test_size=0.1, random_state=1)\n",
        "\n",
        "print(\"Train size:\", len(train_texts))\n",
        "print(\"Dev size:\", len(dev_texts))\n",
        "print(\"Test size:\", len(test_texts))\n"
      ],
      "metadata": {
        "id": "a49MbNS76fC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGpt Response: <br>\n",
        "This code loads the dataset stored in the file specified by CORPUS_PATH, splits it into three subsets (training, development, and testing) using the train_test_split function from Scikit-learn, and prints the sizes of the resulting subsets.\n",
        "\n",
        "The first block of code reads in the data from the file using the ndjson.load function, which is able to parse each line of the file as a separate JSON object. The resulting data object is a list of dictionaries, where each dictionary represents a review and contains two keys: \"title\", which stores the review's title, and \"body\", which stores the main body of the review.\n",
        "\n",
        "The second block of code creates two lists: texts and labels. texts is a list of strings that concatenates the \"title\" and \"body\" fields of each review into a single string. labels is a list of integers representing the rating for each review (on a scale of 1 to 5).\n",
        "\n",
        "The third block of code splits the texts and labels lists into three separate sets: train_texts, dev_texts, and test_texts, and train_labels, dev_labels, and test_labels. The train_test_split function randomly partitions the data, with 90% used for training, and 10% used for testing. The test_size parameter specifies the proportion of the data to be used for testing, while random_state ensures reproducibility of the random partitioning.\n",
        "\n",
        "Finally, the code prints the number of examples in each of the resulting subsets."
      ],
      "metadata": {
        "id": "tr2CZiHIkx4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('lr', LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\"))\n",
        "])\n",
        "\n",
        "parameters = {'lr__C': [0.1, 0.5, 1, 2, 5, 10, 100, 1000]}\n",
        "\n",
        "best_classifier = GridSearchCV(pipeline, parameters, cv=5, verbose=1)\n",
        "best_classifier.fit(train_texts, train_labels)\n",
        "best_predictions = best_classifier.predict(test_texts)\n",
        "\n",
        "baseline_accuracy = np.mean(best_predictions == test_labels)\n",
        "print(\"Baseline accuracy:\", baseline_accuracy)"
      ],
      "metadata": {
        "id": "clwrMCXQk3qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGpt Response: <br>\n",
        "This code defines a logistic regression model for sentiment analysis, and uses grid search to find the best hyperparameter C for the model. The resulting model is evaluated on the test set to obtain a baseline accuracy.\n",
        "\n",
        "The logistic regression model is defined as a Pipeline object that chains three different components:\n",
        "\n",
        "A CountVectorizer, which converts each review text into a sparse matrix of word counts.\n",
        "A TfidfTransformer, which applies term frequency-inverse document frequency (TF-IDF) weighting to the word counts.\n",
        "A LogisticRegression classifier, which uses a one-vs-rest approach (multi_class=\"ovr\") and the limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) solver (solver=\"lbfgs\") to perform multi-class classification.\n",
        "The Pipeline object allows the three components to be treated as a single model that can be trained and used for prediction.\n",
        "\n",
        "The parameters dictionary defines a single hyperparameter C for the logistic regression classifier, and specifies a set of eight possible values to try.\n",
        "\n",
        "The GridSearchCV function is used to perform a grid search over the hyperparameter space defined by parameters. The cv parameter specifies 5-fold cross-validation to estimate the generalization performance of each hyperparameter setting. The verbose parameter is set to 1 to print progress messages during the search.\n",
        "\n",
        "The resulting best_classifier object is the grid search estimator that has been trained on the training set with the best hyperparameters found during the search.\n",
        "\n",
        "The best_predictions array contains the predicted ratings for the test set, which are compared against the true ratings test_labels to compute the baseline accuracy using NumPy's np.mean function.\n",
        "\n",
        "The resulting baseline accuracy provides a measure of the performance of the logistic regression model before any further improvements are made."
      ],
      "metadata": {
        "id": "85Jy9PBwk7YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt\n",
        "!pip install transformers\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BERT_MODEL = \"bert-base-uncased\"\n",
        "from transformers.tokenization_bert import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)"
      ],
      "metadata": {
        "id": "-8PFQb5NlIBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGpt Response: <br>\n",
        "This code installs the transformers library using the !pip install command, and then imports the torch library for working with PyTorch tensors. The device variable is set to \"cuda\" if a GPU is available, and \"cpu\" otherwise. This will be used to move tensors to the appropriate device for computation.\n",
        "\n",
        "The BERT_MODEL variable is set to \"bert-base-uncased\", which is a pre-trained BERT model from the Hugging Face Transformers library that has been trained on lower-cased English text.\n",
        "\n",
        "The BertTokenizer class from the transformers.tokenization_bert module is then imported, and an instance of the tokenizer is created using the from_pretrained method with the BERT_MODEL variable as the argument. This tokenizer will be used to tokenize and encode the review texts for input to the BERT model."
      ],
      "metadata": {
        "id": "XXqNZlA4lKj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "MAX_SEQ_LENGTH=100\n",
        "\n",
        "class BertInputItem(object):\n",
        "    \"\"\"An item with all the necessary attributes for finetuning BERT.\"\"\"\n",
        "\n",
        "    def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.text = text\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "        \n",
        "\n",
        "def convert_examples_to_inputs(example_texts, example_labels, label2idx, max_seq_length, tokenizer, verbose=0):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "    \n",
        "    input_items = []\n",
        "    examples = zip(example_texts, example_labels)\n",
        "    for (ex_index, (text, label)) in enumerate(examples):\n",
        "\n",
        "        # Create a list of token ids\n",
        "        input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n",
        "        if len(input_ids) > max_seq_length:\n",
        "            input_ids = input_ids[:max_seq_length]\n",
        "\n",
        "        # All our tokens are in the first input segment (id 0).\n",
        "        segment_ids = [0] * len(input_ids)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        label_id = label2idx[label]\n",
        "\n",
        "        input_items.append(\n",
        "            BertInputItem(text=text,\n",
        "                          input_ids=input_ids,\n",
        "                          input_mask=input_mask,\n",
        "                          segment_ids=segment_ids,\n",
        "                          label_id=label_id))\n",
        "\n",
        "        \n",
        "    return input_items\n",
        "\n",
        "train_features = convert_examples_to_inputs(train_texts, train_labels, label2idx, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n",
        "dev_features = convert_examples_to_inputs(dev_texts, dev_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = convert_examples_to_inputs(test_texts, test_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "6LZk4w-mlWKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGpt Response: <br>\n",
        "Great job on implementing the convert_examples_to_inputs function! It takes in a list of example texts and their corresponding labels, as well as a dictionary mapping the possible label names to their corresponding index, and converts them into a list of BertInputItems that can be used to train a BERT model.\n",
        "\n",
        "It's worth noting that the function truncates examples that are longer than the maximum sequence length specified by MAX_SEQ_LENGTH and pads shorter examples with zeros. This is a common approach when working with fixed-length inputs, and BERT requires fixed-length inputs to work properly.\n",
        "\n",
        "By calling this function on the training, development, and test datasets, you've created three lists of BertInputItems that can be used to train, tune, and evaluate a BERT model."
      ],
      "metadata": {
        "id": "AFGftlpYlZKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "\n",
        "def get_data_loader(features, max_seq_length, batch_size, shuffle=True): \n",
        "\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
        "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "\n",
        "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\n",
        "dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = get_data_loader(test_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "t_4bvq-alrNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGpt Response: <br>\n",
        "The get_data_loader() function takes in a list of BertInputItem objects, as well as other parameters, and converts them into PyTorch DataLoader objects that can be fed into the neural network for training or evaluation. Specifically, it creates a TensorDataset from the input features, and then creates a DataLoader that iterates over batches of the TensorDataset.\n",
        "\n",
        "In the function, the all_input_ids, all_input_mask, all_segment_ids, and all_label_ids tensors are created from the BertInputItem objects. These tensors correspond to the input token IDs, input mask (indicating which tokens are padding tokens), segment IDs (indicating which tokens belong to which sentence in a pair of sentences), and label IDs (corresponding to the integer indices of the labels in the label2idx mapping), respectively. These tensors are then used to create a TensorDataset, which is a PyTorch dataset that stores these tensors in a way that is amenable to batching.\n",
        "\n",
        "Finally, the DataLoader is created from the TensorDataset, with the batch_size and shuffle parameters specified. The batch_size parameter determines how many examples are processed in each batch, and the shuffle parameter determines whether the examples in each batch are randomly shuffled or not. The resulting DataLoader can then be iterated over to obtain batches of data for feeding into the neural network. "
      ],
      "metadata": {
        "id": "UFKDn4VPluMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#THis is the complex code given by ChatGpt \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import transformers\n",
        "\n",
        "class BERTTextClassificationModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BERTTextClassificationModel, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, pooled_output = self.bert(input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded_text = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return encoded_text['input_ids'], encoded_text['attention_mask'], label\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    for input_ids, attention_mask, labels in train_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in val_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            loss = criterion(logits, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct_preds += torch.sum(preds == labels)\n",
        "            total_preds += len(labels)\n",
        "\n",
        "    val_loss = running_loss / len(val_loader)\n",
        "    val_acc = correct_preds / total_preds\n",
        "    return val_loss, val_acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_texts = ['This is the first text', 'This is the second text', 'This is the third text']\n",
        "    train_labels = [0, 1, 1] # binary labels\n",
        "    val_texts = ['This is the fourth text', 'This is the fifth text']\n",
        "    val_labels = [0, 1] # binary labels\n",
        "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=2)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BERTTextClassificationModel(num_classes\n"
      ],
      "metadata": {
        "id": "V6gwjWN9l_ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGpt Response: <br>\n",
        "It seems like the code is incomplete as the line model = BERTTextClassificationModel(num_classes is not complete. But based on the code provided, it looks like a text classification model using BERT is being implemented.\n",
        "\n",
        "The BERTTextClassificationModel class defines the architecture of the model. It uses the BERT model as the base, with a dropout layer and a linear classifier layer on top to classify text into num_classes categories.\n",
        "\n",
        "The TextClassificationDataset class defines a custom dataset to be used with the DataLoader. It takes in texts, labels, and a tokenizer, and returns the input_ids, attention_mask, and label for each instance in the dataset.\n",
        "\n",
        "The train function trains the model on the given train_loader, using the specified optimizer and criterion. It sets the model to training mode and performs backpropagation to update the weights.\n",
        "\n",
        "The evaluate function evaluates the model on the given val_loader, using the specified criterion. It sets the model to evaluation mode, calculates the validation loss and accuracy, and returns them.\n",
        "\n",
        "In the main function, the code defines train and validation datasets, initializes the data loaders, selects the device for training, initializes the model with the given number of classes, and trains and evaluates the model using the defined functions.\n",
        "\n",
        "Overall, this code implements a simple binary text classification model using BERT and PyTorch."
      ],
      "metadata": {
        "id": "L_-smP_zmPCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So i asked ChatGpt to remove error and improve the code"
      ],
      "metadata": {
        "id": "kQdBcsETmXdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt with improved code\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import transformers\n",
        "\n",
        "class BERTTextClassificationModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BERTTextClassificationModel, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, pooled_output = self.bert(input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded_text = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return encoded_text['input_ids'], encoded_text['attention_mask'], label\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    for input_ids, attention_mask, labels in train_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in val_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            loss = criterion(logits, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct_preds += torch.sum(preds == labels)\n",
        "            total_preds += len(labels)\n",
        "\n",
        "    val_loss = running_loss / len(val_loader)\n",
        "    val_acc = correct_preds / total_preds\n",
        "    return val_loss, val_acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_texts = ['This is the first text', 'This is the second text', 'This is the third text']\n",
        "    train_labels = [0, 1, 1] # binary labels\n",
        "    val_texts = ['This is the fourth text', 'This is the fifth text']\n",
        "    val_labels = [0, 1] # binary labels\n",
        "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=2)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BERTTextClassificationModel\n"
      ],
      "metadata": {
        "id": "7hNHsiOsms51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like there is a typo in the code.\n",
        "\n",
        "In the last line of the code, the model is not instantiated. Instead, it is only being assigned the class BERTTextClassificationModel. To instantiate the model, parentheses need to be added after the class name. The corrected last line should be:\n",
        "\n",
        "scss\n",
        "Copy code\n",
        "model = BERTTextClassificationModel(num_classes=2)\n",
        "This creates an instance of the BERTTextClassificationModel with 2 output classes (binary classification).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-4Tr5w59nkRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##HOW DID CHATGPT HANDLED COMPLEXITY\n",
        "#### After testing ChatGpt for part C. ChatGpt performed pretty good in \n",
        "#### explaining almost all kinds of code. But when the complexity is increased\n",
        "#### the accuracy of CHatGpt starts reducing and responses becomes redundant. "
      ],
      "metadata": {
        "id": "sm4-O17gCjcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART D\n",
        "\n",
        "#### For this part of the assignment, we gave ChatGPT a few broken code snippets and checked if it was able to debug or find errors in the codes.\n",
        "\n",
        "## Task 1: Debugging Example"
      ],
      "metadata": {
        "id": "uHab5L5aXATH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Broke Code 1\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"I like to eat apples and oranges.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    if token.text == \"apples\":\n",
        "        token.text = \"bananas\"\n",
        "\n",
        "print(doc.text)"
      ],
      "metadata": {
        "id": "HpNtyr68XGL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This code attempts to replace the word \"apples\" with \"bananas\" in the input text, using the Spacy library for NLP processing. However, this code will produce an error, because the token.text attribute is read-only and cannot be modified directly.\n",
        "####To fix this code, you would need to create a new string with the modified tokens, rather than trying to modify the tokens themselves.\n",
        "\n",
        "####ChatGPT impressively found the mistake at once and gave the corrected code along with the explanation:\n",
        "\n",
        "## ChatGPT Response"
      ],
      "metadata": {
        "id": "Ldh6t8RoXrF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct Code 1\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"I like to eat apples and oranges.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "new_tokens = []\n",
        "for token in doc:\n",
        "    if token.text == \"apples\":\n",
        "        new_tokens.append(\"bananas\")\n",
        "    else:\n",
        "        new_tokens.append(token.text)\n",
        "\n",
        "new_text = \" \".join(new_tokens)\n",
        "print(new_text)\n"
      ],
      "metadata": {
        "id": "5AhuLciAXqyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Next, we removed an import statement from the broken code and tried to find out if it could detect whether the import statement was removed. \n",
        "####However, this was not the case and ChatGPT gave the same corrected code that it gave in the above case (Corrected Code 1). \n",
        "####This proved that ChatGPT did not detect the missing import statement and only gave the output that it was initially trained to give. It could not detect the newly introduced bug."
      ],
      "metadata": {
        "id": "28tBJl3lYL4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2:Debugging\n",
        "Problematic Code:\n"
      ],
      "metadata": {
        "id": "7r2iu4N5vPNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Load the spaCy English language model and define the stop words\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stop_words = STOP_WORDS\n",
        "\n",
        "# Define a function to preprocess the text\n",
        "def preprocess(text):\n",
        "    # Tokenize the text\n",
        "    doc = nlp(text)\n",
        "    # Remove stop words and punctuation, and lemmatize the tokens\n",
        "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    # Return the preprocessed tokens as a list\n",
        "    return tokens\n",
        "\n",
        "# Define the hotel review\n",
        "\n",
        "# Preprocess the hotel review\n",
        "processed_review = preprocess(review)\n",
        "\n",
        "# Train a Word2Vec model on the hotel review\n",
        "model = Word2Vec([processed_review], min_count=1)\n",
        "\n",
        "# Define a function to perform topic modeling using LDA\n",
        "def topic_modeling(corpus, num_topics):\n",
        "    # Create a dictionary from the corpus\n",
        "    dictionary = Dictionary(corpus)\n",
        "    # Create a bag-of-words representation of the corpus\n",
        "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
        "    # Train an LDA model on the bag-of-words representation\n",
        "    lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary)\n",
        "    # Return the trained LDA model\n",
        "    return lda_model\n",
        "\n",
        "# Perform topic modeling on the hotel review using LDA\n",
        "corpus = [processed_review]\n",
        "num_topics = 1\n",
        "lda_model = topic_modeling(corpus, num_topics)\n",
        "\n",
        "# Print the topics and their associated keywords\n",
        "topics = lda_model.show_topics(num_topics=num_topics, num_words=5, formatted=False)\n",
        "for topic in topics:\n",
        "    print(\"Topic {}: {}\".format(topic[0], [word[0] for word in topic[1]]))"
      ],
      "metadata": {
        "id": "pxtftLaAuX3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT Response:\n",
        "\n",
        "The code appears to be missing the variable review, which is used in the line processed_review = preprocess(review). You need to define the review variable and assign it a string value before running the code.\n",
        "\n",
        "Assuming that review has been defined, the code may also throw an error if the processed_review variable is an empty list after being processed by the preprocess() function. This can happen if the review variable contains only stop words or punctuation.\n",
        "\n",
        "To avoid this, you can add a check to ensure that the processed_review variable is not empty before passing it to the Word2Vec function. For example:\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "processed_review = preprocess(review)\n",
        "if processed_review:\n",
        "    model = Word2Vec([processed_review], min_count=1)\n",
        "else:\n",
        "    print(\"Error: processed_review is empty\")\n",
        "```\n",
        "Other than that, the code looks fine. It loads the spaCy English language model, defines a function to preprocess text by removing stop words and lemmatizing the remaining words, trains a Word2Vec model on the preprocessed hotel review, and performs topic modeling on the review using LDA. Finally, it prints the topics and their associated keywords.\n"
      ],
      "metadata": {
        "id": "gyhgkKHXviUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: debugging snippet of code\n",
        "\n",
        "Problematic code:"
      ],
      "metadata": {
        "id": "AxPe7jWEv2AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of all the aspirational phrases or words list, to replace the comments containing these texts with \"aspiration\"\n",
        "aspirational_words = ['hope that', 'hope to', 'wish to', 'wish that', 'dream to', 'dream that', 'would like', 'would love', 'aim', 'plan to', 'need to', 'needs to']\n",
        "aspiration = pd.DataFrame(aspirational_words)\n",
        "aspiration.columns=['phrases']\n",
        "aspiration_list = aspiration['phrases'].to_list()\n",
        "\n",
        "for i in aspirational_words:\n",
        "    df['comments_aspiration'] = df['comments'].str.replace(i,\"aspiration\")\n",
        "\n",
        "df['comments_tokens'] = df['comments_aspiration'].apply(lambda each_post: word_tokenize(re.sub(r'[^\\w\\s]',' ',each_post.lower())))\n",
        "df['comments_tokens'] = df['comments_tokens'].apply(lambda list_of_words: [x if x not in model_brand_dict else model_brand_dict[x] for x in list_of_words])\n",
        "df['comments_tokens'] = df['comments_tokens'].apply(lambda list_of_words: [x for x in list_of_words if x not in stop])\n",
        "df['comments_unique'] = df['comments_tokens'].apply(lambda x: sorted(set(x)))\n",
        "# Lift: features and brands\n",
        "lift_aspiration_brand = pd.DataFrame(columns = ['brand','feature','lift'])\n",
        "i=0\n",
        "for brand in brands:\n",
        "    temp_lift=calculate_lift(str(brand), df.comments_unique.contains('aspiratio'), df.comments_unique)\n",
        "    lift_aspiration_brand.loc[i]=[str(brand),str(df.comments_unique).contains('aspiratio'),temp_lift]\n",
        "    i=i+1\n",
        "\n",
        "lift_aspiration_brand.sort_values('lift',ascending = False).head(10)"
      ],
      "metadata": {
        "id": "vQitXxmbw3HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT Response:\n",
        "\n",
        "There are several issues with the code:\n",
        "\n",
        "pd module is not imported. Add import pandas as pd at the beginning of the code.\n",
        "\n",
        "df dataframe is not defined in the code. You need to define and populate df with the comments data before running the code.\n",
        "\n",
        "The line df['comments_aspiration'] = df['comments'].str.replace(i,\"aspiration\") is inside the for loop but should be outside. Otherwise, it will only replace the last aspirational word in the list.\n",
        "\n",
        "df.comments_unique.contains('aspiratio') and str(df.comments_unique).contains('aspiratio') should be replaced with df.comments_tokens.apply(lambda x: 'aspiration' in x) because contains is not a method of a Python list object.\n",
        "\n",
        "calculate_lift function is not defined in the code. You need to define it before using it.\n",
        "\n",
        "Here's the corrected code:\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# Define df with comments data\n",
        "df = pd.DataFrame({'comments': ['I hope that I can buy this product', 'I wish to have this product', 'This is my dream to own', 'I would like to purchase this', 'I plan to buy this', 'I need to have this']})\n",
        "\n",
        "# Create a list of all the aspirational phrases or words list, to replace the comments containing these texts with \"aspiration\"\n",
        "aspirational_words = ['hope that', 'hope to', 'wish to', 'wish that', 'dream to', 'dream that', 'would like', 'would love', 'aim', 'plan to', 'need to', 'needs to']\n",
        "aspiration_list = ['aspiration']\n",
        "\n",
        "for i in aspirational_words:\n",
        "    df['comments'] = df['comments'].str.replace(i, \"aspiration\")\n",
        "\n",
        "df['comments_tokens'] = df['comments'].apply(lambda each_post: word_tokenize(re.sub(r'[^\\w\\s]',' ',each_post.lower())))\n",
        "df['comments_tokens'] = df['comments_tokens'].apply(lambda list_of_words: [x if x not in model_brand_dict else model_brand_dict[x] for x in list_of_words])\n",
        "df['comments_tokens'] = df['comments_tokens'].apply(lambda list_of_words: [x for x in list_of_words if x not in stop])\n",
        "df['comments_unique'] = df['comments_tokens'].apply(lambda x: sorted(set(x)))\n",
        "\n",
        "# Lift: features and brands\n",
        "lift_aspiration_brand = pd.DataFrame(columns=['brand', 'feature', 'lift'])\n",
        "i=0\n",
        "for brand in brands:\n",
        "    temp_lift = calculate_lift(str(brand), df.comments_tokens.apply(lambda x: 'aspiration' in x), df.comments_unique)\n",
        "    lift_aspiration_brand.loc[i] = [str(brand), 'aspiration', temp_lift]\n",
        "    i += 1\n",
        "\n",
        "lift_aspiration_brand.sort_values('lift', ascending=False).head(10)\n",
        "```\n",
        "Note: You will need to define brands, stop, and model_brand_dict variables and the calculate_lift function before running the corrected code.\n"
      ],
      "metadata": {
        "id": "y-uBNOZxw_gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Sentiment Analysis\n",
        "\n",
        "Problematic Code:"
      ],
      "metadata": {
        "id": "_K3RDq3y5xzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Imports\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "#Created dataframe from scraped tweets\n",
        "df = pd.read_csv(\"tweetsv3.csv\", names=[\"username\", \"location\", \"text\"])\n",
        "#Remove all na's\n",
        "df = df.fillna('NA')\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "#Function to categorize the compound value as positive, negative, or neutral\n",
        "def senti(df):\n",
        "    if df['compound'] >= 0.05:\n",
        "        val = \"Positive\"\n",
        "    elif df['compound'] <= -0.05:\n",
        "        val = \"Negative\"\n",
        "    else:\n",
        "        val = \"Neutral\"\n",
        "    return val\n",
        "\n",
        "#Calling function  \n",
        "df['Comp_score'] = df.apply(senti, axis=1)\n",
        "#Function to implement sentiment analysis by running loop through text token column and looking for issue and candidate using the window=10 function\n",
        "def sentimentIssue(df,issue, candidate, window):\n",
        "    scores=[]\n",
        "    for i in df[\"text_tokens\"]:\n",
        "        if ((issue in d) and (candidate in i)):\n",
        "            a = ' '.join(e for e in i[i.index(issue)-window:i.index(issue)+window])\n",
        "            if a != \"\":\n",
        "                sentiment_score = analyser.polarity_scores(a)['compound']\n",
        "                scores.append(sentiment_score)\n",
        "    return np.mean(scores)\n",
        "#Printing sentiment values\n",
        "print(\"Sentiment between abortion and Oz is negative and the sentiment score is %.4f\"% sentimentIssue(df,\"abortion\", \"oz\", window=10))\n",
        "print(\"Sentiment between crime and Oz is negative and the sentiment score is %.4f\"% sentimentIssue(df,\"crime\", \"oz\", window=10))\n",
        "print(\"Sentiment between tax and Oz is neutral and the sentiment score is %.4f\"% sentimentIssue(df,\"tax\", \"oz\", window=10))\n",
        "print(\"Sentiment between foreign and Oz is positive and the sentiment score is %.4f\"% sentimentIssue(df,\"foreign\", \"oz\", window=10))\n",
        "print(\"Sentiment between abortion and Fetterman is negative and the sentiment score is %.4f\"% sentimentIssue(df,\"abortion\", \"fetterman\", window=10))\n",
        "print(\"Sentiment between crime and Fetterman is negative and the sentiment score is %.4f\"% sentimentIssue(df,\"crime\", \"fetterman\", window=10))\n",
        "print(\"Sentiment between tax and Fetterman is neutral and the sentiment score is %.4f\"% sentimentIssue(df,\"tax\", \"fetterman\", window=10))\n",
        "print(\"Sentiment between foreign and Fetterman is positive and the sentiment score is %.4f\"% sentimentIssue(df,\"foreign\", \"fetterman\", window=10))\n",
        "\n"
      ],
      "metadata": {
        "id": "T7lUUxzo-Qr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT Response:\n",
        "\n",
        "There are a few issues with the provided code:\n",
        "\n",
        "The import statement for the NumPy library is missing, but it is required for the code to run. You can add it at the top of the code with import numpy as np.\n",
        "\n",
        "In the sentimentIssue function, there is a typo where d should be i. The corrected line should be if ((issue in i) and (candidate in i)):.\n",
        "\n",
        "In the sentimentIssue function, the scores list is not initialized, which can cause an error if no sentiment scores are found. You can initialize it as an empty list at the beginning of the function with scores = [].\n",
        "\n",
        "Here's the corrected code:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "## Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "#Created dataframe from scraped tweets\n",
        "df = pd.read_csv(\"tweetsv3.csv\", names=[\"username\", \"location\", \"text\"])\n",
        "#Remove all na's\n",
        "df = df.fillna('NA')\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "#Function to categorize the compound value as positive, negative, or neutral\n",
        "def senti(df):\n",
        "    if df['compound'] >= 0.05:\n",
        "        val = \"Positive\"\n",
        "    elif df['compound'] <= -0.05:\n",
        "        val = \"Negative\"\n",
        "    else:\n",
        "        val = \"Neutral\"\n",
        "    return val\n",
        "\n",
        "#Calling function  \n",
        "df['Comp_score'] = df.apply(senti, axis=1)\n",
        "\n",
        "#Function to implement sentiment analysis by running loop through text token column and looking for issue and candidate using the window=10 function\n",
        "def sentimentIssue(df, issue, candidate, window):\n",
        "    scores = []\n",
        "    for i in df[\"text_tokens\"]:\n",
        "        if ((issue in i) and (candidate in i)):\n",
        "            a = ' '.join(e for e in i[i.index(issue)-window:i.index(issue)+window])\n",
        "            if a != \"\":\n",
        "                sentiment_score = analyser.polarity_scores(a)['compound']\n",
        "                scores.append(sentiment_score)\n",
        "    return np.mean(scores)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "maSX1dgE-w6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis: \n",
        "\n",
        "ChatGPT is a large language model trained by OpenAI. It provides guidance on the process of debugging code and suggests potential sources of errors based on its knowledge of programming concepts and syntax. ChatGPT can suggest potential syntax errors, logical errors, or algorithmic errors that may be causing the problem, and can provide information on debugging techniques and tools that can be used to find and fix the error. However, the extent to which ChatGPT can detect a bug in the code depends on the complexity of the code, the nature of the error, and the accuracy and completeness of the information provided to it. Ultimately, the effectiveness of the debugging process relies on the skill and experience of the programmer, as well as the tools and resources available to them. \n",
        "\n",
        "In task 3, ChatGPT Failed to fill in missing lines of code that is needed to sort the colums of the dataframe needed for the sentiment analysis. The code missed is as follow:\n",
        "\n",
        "```\n",
        "df['scores'] = df['text'].apply(lambda text: analyser.polarity_scores(text))\n",
        "df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "```\n"
      ],
      "metadata": {
        "id": "M8HdqJsc_IB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part E\n"
      ],
      "metadata": {
        "id": "m_Eay6BfkTY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### As an AI language model, ChatGPT is designed to generate responses to user queries, and provide information and suggestions based on a vast amount of data and training. In terms of its competence level, ChatGPT is significantly more advanced than a novice in the tasks it is designed to perform. However, it may not yet reach the level of expertise in certain areas that require specific domain knowledge.\n",
        "####In comparison to a human expert, ChatGPT may lack the ability to understand and interpret complex social cues, nuances, and contextual information that require high-level cognitive skills, emotional intelligence, and experience. However, it can process and analyze vast amounts of data and provide insights and suggestions based on its training and previous interactions.\n",
        "####To improve ChatGPT's performance in a real-world enterprise setting, there are several areas that need improvement. One of the main areas is to enhance its ability to understand and respond appropriately to the context of the conversation. This could be achieved by training the model with more contextual data and providing it with better natural language processing capabilities. Additionally, ChatGPT's responses could be fine-tuned by using real-time feedback from users to improve accuracy and relevance.\n",
        "####Another area of improvement for ChatGPT is to expand its knowledge and expertise in specific domains. This could be achieved by training the model with more domain-specific data and incorporating subject matter experts' input to ensure accuracy and relevancy.\n",
        "####Finally, to perform better in a real-world enterprise setting, ChatGPT needs to be designed to handle various data formats, such as images, audio, and video. This would enable the model to provide more comprehensive and relevant responses to users' queries.\n",
        "####In conclusion, ChatGPT's level of competence is advanced, and it outperforms a novice in the tasks it is designed to perform. However, there is still room for improvement in certain areas to enhance its accuracy, relevancy, and ability to handle complex social cues and contextual information. Achieving these improvements will require ongoing training, fine-tuning, and integration of real-time feedback from users to continuously improve its performance in a real-world enterprise setting."
      ],
      "metadata": {
        "id": "8ZrlhblckVmh"
      }
    }
  ]
}